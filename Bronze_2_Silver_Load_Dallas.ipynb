{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e3f87f67-6125-4ff4-af9f-cd5166305c77",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import dlt\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import current_timestamp\n",
    "\n",
    "# --- Dallas Bronze ---\n",
    "@dlt.table(\n",
    "    name=\"midterm_project.bronze.dallas_bronze\",\n",
    "    comment=\"Raw Dallas inspection data\"\n",
    ")\n",
    "def dallas_bronze():\n",
    "    df = (\n",
    "        spark.read.format(\"csv\")\n",
    "        .option(\"header\", True)\n",
    "        .option(\"delimiter\", \"\\t\")\n",
    "        .option(\"multiLine\", True)\n",
    "        .load(\"/Volumes/midterm_project/raw/d_store/Dallas_Raw.tsv\")\n",
    "        .withColumn(\"source_city\", lit(\"Dallas\"))\n",
    "        .withColumn(\"load_dt\", current_timestamp())\n",
    "    )\n",
    "    # Sanitize column names: replace spaces with underscores\n",
    "    df = df.toDF(*[c.replace(\" \", \"_\") for c in df.columns])\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e3be2d6e-a7e2-409e-bc12-857d8246d1ef",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import dlt\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.window import Window\n",
    "from functools import reduce\n",
    "import operator\n",
    "\n",
    "\n",
    "@dlt.table(\n",
    "    name=\"midterm_project.silver.silver_dallas\",\n",
    "    comment=\"Cleansed and standardized Dallas Food Inspection Data (Silver Layer)\",\n",
    "    table_properties={\"quality\": \"silver\"}\n",
    ")\n",
    "@dlt.expect_or_drop(\"not_null_business_name\", \"BUSINESS_NAME IS NOT NULL\")\n",
    "@dlt.expect_or_drop(\"not_null_inspection_date\", \"INSPECTION_DATE IS NOT NULL\")\n",
    "@dlt.expect_or_drop(\"not_null_inspection_type\", \"INSPECTION_TYPE IS NOT NULL\")\n",
    "@dlt.expect_or_drop(\"valid_zip_code\", \"ZIP_CODE IS NOT NULL AND LENGTH(ZIP_CODE) = 5\")\n",
    "@dlt.expect_or_drop(\"score_within_range\", \"INSPECTION_SCORE >= 0 AND INSPECTION_SCORE <= 100\")\n",
    "@dlt.expect_or_drop(\"high_score_low_violations\", \"INSPECTION_SCORE < 90 OR VIOLATION_COUNT <= 3\")\n",
    "@dlt.expect_or_drop(\"min_one_violation\", \"VIOLATION_COUNT >= 1\")\n",
    "@dlt.expect_or_drop(\"no_pass_with_critical_urgent\", \"NOT (INSPECTION_SCORE >= 70 AND HAS_CRITICAL_URGENT = TRUE)\")\n",
    "@dlt.expect(\"unique_inspection_id\", \"INSPECTION_ID IS NOT NULL\")\n",
    "def silver_dallas():\n",
    "    \"\"\"\n",
    "    Transform Dallas inspections from Bronze to Silver with comprehensive cleaning\n",
    "    Drops rows where SCORE >= 70 AND any violation contains \"CRITICAL\" or \"URGENT\"\n",
    "    \"\"\"\n",
    "    \n",
    "    df = dlt.read(\"dallas_bronze\")\n",
    "    \n",
    "    # STEP 1: RENAME COLUMNS - Remove spaces and standardize\n",
    "    column_mapping = {col: col.strip().replace(' ', '_').upper() for col in df.columns}\n",
    "    for old_col, new_col in column_mapping.items():\n",
    "        df = df.withColumnRenamed(old_col, new_col)\n",
    "    \n",
    "    # STEP 2: PARSE LAT_LONG_LOCATION - Replace non-Dallas coords with ZIP center\n",
    "    lat_long_col = None\n",
    "    for col_name in ['LAT_LONG_LOCATION', 'LOCATION', 'LAT_LONG', 'GEOLOCATION']:\n",
    "        if col_name in df.columns:\n",
    "            lat_long_col = col_name\n",
    "            break\n",
    "    \n",
    "    if lat_long_col:\n",
    "        df = df.withColumn(\"LATITUDE_TEMP\", F.regexp_extract(F.col(lat_long_col), r'\\(([0-9.-]+),\\s*([0-9.-]+)\\)', 1).cast(DoubleType()))\n",
    "        df = df.withColumn(\"LONGITUDE_TEMP\", F.regexp_extract(F.col(lat_long_col), r'\\(([0-9.-]+),\\s*([0-9.-]+)\\)', 2).cast(DoubleType()))\n",
    "        \n",
    "        zip_window = Window.partitionBy(\"ZIP_CODE\")\n",
    "        df = df.withColumn(\"ZIP_LAT_MEAN\", F.avg(\"LATITUDE_TEMP\").over(zip_window))\n",
    "        df = df.withColumn(\"ZIP_LON_MEAN\", F.avg(\"LONGITUDE_TEMP\").over(zip_window))\n",
    "        \n",
    "        df = df.withColumn(\"LATITUDE\",\n",
    "                          F.when((F.col(\"LATITUDE_TEMP\").isNull()) | (F.col(\"LATITUDE_TEMP\") < 32.5) | (F.col(\"LATITUDE_TEMP\") > 33.2),\n",
    "                                F.coalesce(F.col(\"ZIP_LAT_MEAN\"), F.lit(32.7767))).otherwise(F.col(\"LATITUDE_TEMP\")))\n",
    "        \n",
    "        df = df.withColumn(\"LONGITUDE\",\n",
    "                          F.when((F.col(\"LONGITUDE_TEMP\").isNull()) | (F.col(\"LONGITUDE_TEMP\") < -97.5) | (F.col(\"LONGITUDE_TEMP\") > -96.5),\n",
    "                                F.coalesce(F.col(\"ZIP_LON_MEAN\"), F.lit(-96.7970))).otherwise(F.col(\"LONGITUDE_TEMP\")))\n",
    "        \n",
    "        df = df.drop(\"LATITUDE_TEMP\", \"LONGITUDE_TEMP\", \"ZIP_LAT_MEAN\", \"ZIP_LON_MEAN\")\n",
    "    else:\n",
    "        df = df.withColumn(\"LATITUDE\", F.lit(32.7767))\n",
    "        df = df.withColumn(\"LONGITUDE\", F.lit(-96.7970))\n",
    "    \n",
    "    # STEP 3: DATA TYPE CONVERSIONS\n",
    "    if 'ZIP_CODE' in df.columns:\n",
    "        df = df.withColumn(\"ZIP_CODE\", \n",
    "                          F.when(F.col(\"ZIP_CODE\").isNotNull(),\n",
    "                                F.lpad(F.regexp_replace(F.col(\"ZIP_CODE\").cast(\"string\"), r'[^\\d]', ''), 5, '0').cast(IntegerType()))\n",
    "                          .otherwise(None))\n",
    "    \n",
    "    if 'STREET_NUMBER' in df.columns:\n",
    "        df = df.withColumn(\"STREET_NUMBER\",\n",
    "                          F.when(F.col(\"STREET_NUMBER\").isNotNull(),\n",
    "                                F.regexp_replace(F.col(\"STREET_NUMBER\").cast(\"string\"), r'[^\\d]', '').cast(IntegerType()))\n",
    "                          .otherwise(None))\n",
    "    \n",
    "    if 'INSPECTION_DATE' in df.columns:\n",
    "        df = df.withColumn(\"INSPECTION_DATE\", F.to_date(F.col(\"INSPECTION_DATE\")))\n",
    "    \n",
    "    score_found = False\n",
    "    for score_col in ['SCORE', 'INSPECTION_SCORE', 'TOTAL_SCORE']:\n",
    "        if score_col in df.columns:\n",
    "            df = df.withColumn(\"INSPECTION_SCORE\", F.coalesce(F.col(score_col).cast(IntegerType()), F.lit(0)))\n",
    "            score_found = True\n",
    "            break\n",
    "    if not score_found:\n",
    "        df = df.withColumn(\"INSPECTION_SCORE\", F.lit(0))\n",
    "    \n",
    "    # STEP 4: HANDLE STREET_ADDRESS\n",
    "    address_col = None\n",
    "    for col_name in ['STREET_ADDRESS', 'ADDRESS_LINE1', 'ADDRESS', 'ADDRESS_LINE_1', 'STREET_ADDR']:\n",
    "        if col_name in df.columns:\n",
    "            address_col = col_name\n",
    "            break\n",
    "    \n",
    "    if address_col:\n",
    "        if address_col != 'STREET_ADDRESS':\n",
    "            df = df.withColumnRenamed(address_col, 'STREET_ADDRESS')\n",
    "        df = df.withColumn(\"STREET_ADDRESS\",\n",
    "                          F.when((F.col(\"STREET_ADDRESS\").isNull()) | (F.trim(F.col(\"STREET_ADDRESS\")) == \"\"),\n",
    "                                F.concat(F.lit(\"Address in ZIP \"), F.col(\"ZIP_CODE\"))).otherwise(F.col(\"STREET_ADDRESS\")))\n",
    "    else:\n",
    "        df = df.withColumn(\"STREET_ADDRESS\", F.concat(F.lit(\"Address in ZIP \"), F.col(\"ZIP_CODE\")))\n",
    "    \n",
    "    # STEP 5: RENAME RESTAURANT_NAME TO BUSINESS_NAME AND CREATE AKA_NAME\n",
    "    restaurant_col = None\n",
    "    for col_name in ['RESTAURANT_NAME', 'BUSINESS_NAME', 'FACILITY_NAME', 'ESTABLISHMENT_NAME']:\n",
    "        if col_name in df.columns:\n",
    "            restaurant_col = col_name\n",
    "            break\n",
    "    \n",
    "    if restaurant_col and restaurant_col != 'BUSINESS_NAME':\n",
    "        df = df.withColumnRenamed(restaurant_col, 'BUSINESS_NAME')\n",
    "    elif not restaurant_col:\n",
    "        df = df.withColumn('BUSINESS_NAME', F.lit('Unknown'))\n",
    "    \n",
    "    # Create AKA_NAME as copy of BUSINESS_NAME\n",
    "    df = df.withColumn(\"AKA_NAME\", F.col(\"BUSINESS_NAME\"))\n",
    "    \n",
    "    # Add FACILITY_TYPE column with value \"Restaurant\"\n",
    "    df = df.withColumn(\"FACILITY_TYPE\", F.lit(\"Restaurant\"))\n",
    "    \n",
    "    # Add STATE column with value \"TX\"\n",
    "    df = df.withColumn(\"STATE\", F.lit(\"TX\"))\n",
    "    \n",
    "    # Add CITY column with value \"Dallas\"\n",
    "    df = df.withColumn(\"CITY\", F.lit(\"Dallas\"))\n",
    "    \n",
    "    # STEP 6: REMOVE UNNECESSARY COLUMNS\n",
    "    columns_to_drop = ['INSPECTION_MONTH', 'INSPECTION_YEAR', 'ADDRESS_LINE1']\n",
    "    for col in columns_to_drop:\n",
    "        if col in df.columns:\n",
    "            df = df.drop(col)\n",
    "    \n",
    "    # STEP 7: HANDLE NULL VALUES\n",
    "    string_null_replacement = {\n",
    "        'BUSINESS_NAME': 'Unknown', 'AKA_NAME': 'Unknown', 'STREET_ADDRESS': 'Not Available', \n",
    "        'ADDRESS_LINE2': '', 'CITY': 'Dallas', 'STATE': 'TX', \n",
    "        'FACILITY_TYPE': 'Restaurant', 'INSPECTION_TYPE': 'Unknown'\n",
    "    }\n",
    "    for col, replacement in string_null_replacement.items():\n",
    "        if col in df.columns:\n",
    "            df = df.withColumn(col, F.when((F.col(col).isNull()) | (F.trim(F.col(col)) == \"\"), F.lit(replacement)).otherwise(F.col(col)))\n",
    "    \n",
    "    numeric_null_replacement = {'INSPECTION_SCORE': 0}\n",
    "    for col, replacement in numeric_null_replacement.items():\n",
    "        if col in df.columns:\n",
    "            df = df.withColumn(col, F.coalesce(F.col(col), F.lit(replacement)))\n",
    "    \n",
    "    # STEP 8: COUNT VIOLATIONS\n",
    "    violation_desc_cols = [col for col in df.columns if 'VIOLATION_DESCRIPTION' in col]\n",
    "    if violation_desc_cols:\n",
    "        violation_count_expr = reduce(operator.add,\n",
    "            [F.when((F.col(col).isNotNull()) & (F.trim(F.col(col)) != \"\") & (F.trim(F.col(col)) != \"N/A - Not Applicable\"), F.lit(1)).otherwise(F.lit(0))\n",
    "             for col in violation_desc_cols])\n",
    "        df = df.withColumn(\"VIOLATION_COUNT\", violation_count_expr)\n",
    "    else:\n",
    "        df = df.withColumn(\"VIOLATION_COUNT\", F.lit(0))\n",
    "    \n",
    "    # STEP 9: CHECK FOR \"CRITICAL\" OR \"URGENT\" IN ANY VIOLATION COLUMN\n",
    "    violation_all_cols = [col for col in df.columns if 'VIOLATION_DESCRIPTION' in col or 'VIOLATION_DETAIL' in col or 'VIOLATION_MEMO' in col]\n",
    "    \n",
    "    if violation_all_cols:\n",
    "        has_critical_urgent_conditions = []\n",
    "        for col in violation_all_cols:\n",
    "            has_critical_urgent_conditions.append(\n",
    "                (F.upper(F.col(col)).contains(\"CRITICAL\")) |\n",
    "                (F.upper(F.col(col)).contains(\"URGENT\"))\n",
    "            )\n",
    "        \n",
    "        combined_condition = reduce(operator.or_, has_critical_urgent_conditions)\n",
    "        df = df.withColumn(\"HAS_CRITICAL_URGENT\", F.when(combined_condition, F.lit(True)).otherwise(F.lit(False)))\n",
    "    else:\n",
    "        df = df.withColumn(\"HAS_CRITICAL_URGENT\", F.lit(False))\n",
    "    \n",
    "    # STEP 10: CREATE LOCATION COLUMN\n",
    "    df = df.withColumn(\"LOCATION\", F.concat(F.lit(\"(\"), F.col(\"LATITUDE\").cast(\"string\"), F.lit(\", \"), F.col(\"LONGITUDE\").cast(\"string\"), F.lit(\")\")))\n",
    "    df = df.drop(\"LATITUDE\", \"LONGITUDE\")\n",
    "    \n",
    "    # STEP 11: CALCULATE RISK LEVEL (keeping this for now as it may be used elsewhere)\n",
    "    df = df.withColumn(\"RISK_LEVEL\",\n",
    "                      F.when((F.col(\"INSPECTION_SCORE\") < 70) | (F.col(\"VIOLATION_COUNT\") >= 5), F.lit(\"HIGH\"))\n",
    "                      .when((F.col(\"INSPECTION_SCORE\") >= 70) & (F.col(\"INSPECTION_SCORE\") <= 85) | ((F.col(\"VIOLATION_COUNT\") >= 2) & (F.col(\"VIOLATION_COUNT\") < 5)), F.lit(\"MEDIUM\"))\n",
    "                      .otherwise(F.lit(\"LOW\")))\n",
    "    \n",
    "    # STEP 12: REMOVE DUPLICATES\n",
    "    dedup_columns = ['BUSINESS_NAME', 'INSPECTION_DATE', 'STREET_ADDRESS', 'ZIP_CODE']\n",
    "    dedup_columns = [col for col in dedup_columns if col in df.columns]\n",
    "    if dedup_columns:\n",
    "        df = df.dropDuplicates(dedup_columns)\n",
    "    \n",
    "    # STEP 13: CREATE LICENSE_NUMBER AND UNIQUE INSPECTION_ID\n",
    "    # Generate LICENSE_NUMBER as a 9-digit unique identifier for each business\n",
    "    df = df.withColumn(\"ROW_ID\", F.monotonically_increasing_id())\n",
    "    df = df.withColumn(\"LICENSE_NUMBER\",\n",
    "                      F.lpad(\n",
    "                          (F.abs(F.crc32(F.concat_ws(\"_\", F.col(\"BUSINESS_NAME\"), F.col(\"ZIP_CODE\").cast(\"string\"), F.col(\"STREET_ADDRESS\")))) % 1000000000).cast(\"string\"),\n",
    "                          9,\n",
    "                          '0'\n",
    "                      ).cast(LongType()))\n",
    "    \n",
    "    df = df.withColumn(\"INSPECTION_ID\",\n",
    "                      F.abs(F.crc32(F.concat_ws(\"_\", F.col(\"BUSINESS_NAME\"), F.col(\"INSPECTION_DATE\").cast(\"string\"),\n",
    "                                               F.col(\"ZIP_CODE\").cast(\"string\"), F.col(\"STREET_ADDRESS\"), F.col(\"ROW_ID\").cast(\"string\")))))\n",
    "    df = df.drop(\"ROW_ID\")\n",
    "    \n",
    "    # STEP 14: ADD METADATA\n",
    "    df = df.withColumn(\"source_system\", F.lit(\"Dallas_OpenData\"))\n",
    "    df = df.withColumn(\"pipeline_run_id\", F.current_timestamp())\n",
    "    \n",
    "    # STEP 15: REORDER COLUMNS (temporary - will be cleaned up in violations table)\n",
    "    ordered_cols = [\"INSPECTION_ID\", \"LICENSE_NUMBER\", \"BUSINESS_NAME\", \"AKA_NAME\", \"FACILITY_TYPE\", \"INSPECTION_DATE\", \n",
    "                   \"INSPECTION_TYPE\", \"STREET_ADDRESS\", \"CITY\", \"STATE\", \"ZIP_CODE\", \"LOCATION\", \n",
    "                   \"INSPECTION_SCORE\", \"VIOLATION_COUNT\", \"RISK_LEVEL\", \"HAS_CRITICAL_URGENT\"]\n",
    "    remaining_cols = [col for col in df.columns if col not in ordered_cols]\n",
    "    final_cols = [col for col in ordered_cols if col in df.columns] + remaining_cols\n",
    "    df = df.select(*final_cols)\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "@dlt.table(\n",
    "    name=\"midterm_project.silver.dallas_violations_silver\",\n",
    "    comment=\"Normalized and cleaned violation records - ONE ROW PER VIOLATION with SHARED INSPECTION_ID\",\n",
    "    table_properties={\"quality\": \"silver\", \"pipelines.autoOptimize.zOrderCols\": \"inspection_id\"}\n",
    ")\n",
    "@dlt.expect_or_drop(\"valid_violation\", \"VIOLATION_DESCRIPTION != 'N/A - Not Applicable' AND VIOLATION_DESCRIPTION != 'Unknown' AND VIOLATION_DESCRIPTION != ''\")\n",
    "@dlt.expect_or_drop(\"not_null_violation_desc\", \"VIOLATION_DESCRIPTION IS NOT NULL\")\n",
    "@dlt.expect(\"unique_inspection_id\", \"INSPECTION_ID IS NOT NULL\")\n",
    "def dallas_violations_silver():\n",
    "    \"\"\"\n",
    "    EXPLODED VIEW: One row per violation with final schema\n",
    "    - Drops columns: VIOLATION_COUNT, VIOLATION_ID, HAS_CRITICAL_URGENT, RISK_SCORE, \n",
    "                     INGESTION_TIMESTAMP, VIOLATION_DETAIL, VIOLATION_MEMO, VIOLATION_POINTS, VIOLATION_NUMBER\n",
    "    - Renames: VIOLATION_CODE_EXTRACTED -> VIOLATION_CODE\n",
    "    - Keeps only VIOLATION_DESCRIPTION (uppercase from VIOLATION_DESCRIPTION_NEW)\n",
    "    - Adds INSPECTION_RESULT based on score ranges\n",
    "    - Includes LICENSE_NUMBER\n",
    "    \"\"\"\n",
    "    df = dlt.read(\"midterm_project.silver.silver_dallas\")\n",
    "    \n",
    "    if \"LOCATION\" not in df.columns:\n",
    "        raise Exception(\"LOCATION column not found in silver_dallas!\")\n",
    "    \n",
    "    df_cols = set(df.columns)\n",
    "    violation_arrays = []\n",
    "    \n",
    "    for i in range(1, 26):\n",
    "        violation_number = F.lit(i)\n",
    "        violation_desc_col = f\"VIOLATION_DESCRIPTION_-_{i}\"\n",
    "        violation_detail_col = f\"VIOLATION_DETAIL_-_{i}\"\n",
    "        violation_memo_col = f\"VIOLATION_MEMO_-_{i}\"\n",
    "        violation_points_col = f\"VIOLATION_POINTS_-_{i}\"\n",
    "\n",
    "        if violation_desc_col in df_cols:\n",
    "            violation_desc = F.when((F.trim(F.col(violation_desc_col)) == \"\") | (F.trim(F.col(violation_desc_col)) == \"N/A - Not Applicable\") | (F.col(violation_desc_col).isNull()),\n",
    "                                   F.lit(\"Unknown\")).otherwise(F.col(violation_desc_col))\n",
    "        else:\n",
    "            violation_desc = F.lit(\"N/A - Not Applicable\")\n",
    "        \n",
    "        if violation_detail_col in df_cols:\n",
    "            violation_details = F.when((F.trim(F.col(violation_detail_col)) == \"\") | (F.col(violation_detail_col).isNull()),\n",
    "                                      F.lit(\"Unknown\")).otherwise(F.col(violation_detail_col))\n",
    "        else:\n",
    "            violation_details = F.lit(\"Unknown\")\n",
    "        \n",
    "        if violation_memo_col in df_cols:\n",
    "            violation_memo = F.when((F.trim(F.col(violation_memo_col)) == \"\") | (F.col(violation_memo_col).isNull()),\n",
    "                                   F.lit(\"Unknown\")).otherwise(F.col(violation_memo_col))\n",
    "        else:\n",
    "            violation_memo = F.lit(\"Unknown\")\n",
    "        \n",
    "        if violation_points_col in df_cols:\n",
    "            violation_points = F.coalesce(F.col(violation_points_col).cast(IntegerType()), F.lit(-1))\n",
    "        else:\n",
    "            violation_points = F.lit(-1)\n",
    "\n",
    "        violation_struct = F.struct(\n",
    "            violation_number.alias(\"violation_number\"),\n",
    "            violation_desc.alias(\"violation_description\"),\n",
    "            violation_details.alias(\"violation_details\"),\n",
    "            violation_memo.alias(\"violation_memo\"),\n",
    "            violation_points.alias(\"violation_points\")\n",
    "        )\n",
    "        violation_arrays.append(violation_struct)\n",
    "\n",
    "    df = df.withColumn(\"violations_array\", F.array(*violation_arrays))\n",
    "\n",
    "    key_cols = [\"INSPECTION_ID\", \"LICENSE_NUMBER\", \"BUSINESS_NAME\", \"AKA_NAME\", \"FACILITY_TYPE\", \"INSPECTION_DATE\", \n",
    "               \"INSPECTION_TYPE\", \"STREET_ADDRESS\", \"CITY\", \"STATE\", \"ZIP_CODE\", \"LOCATION\", \n",
    "               \"INSPECTION_SCORE\", \"RISK_LEVEL\"]\n",
    "    existing_key_cols = [col for col in key_cols if col in df.columns]\n",
    "    df_key = df.select(existing_key_cols + [\"violations_array\"])\n",
    "    \n",
    "    df_exploded = df_key.withColumn(\"violation\", F.explode(\"violations_array\"))\n",
    "    \n",
    "    available_cols = df_exploded.columns\n",
    "    select_cols = []\n",
    "    preferred_cols = [\"INSPECTION_ID\", \"LICENSE_NUMBER\", \"BUSINESS_NAME\", \"AKA_NAME\", \"FACILITY_TYPE\", \"INSPECTION_DATE\", \n",
    "                     \"INSPECTION_TYPE\", \"STREET_ADDRESS\", \"CITY\", \"STATE\", \"ZIP_CODE\", \"LOCATION\", \n",
    "                     \"INSPECTION_SCORE\", \"RISK_LEVEL\"]\n",
    "    \n",
    "    for col in preferred_cols:\n",
    "        if col in available_cols:\n",
    "            select_cols.append(col)\n",
    "    \n",
    "    select_cols.extend([\n",
    "        F.col(\"violation.violation_description\").alias(\"VIOLATION_DESCRIPTION_TEMP\"),\n",
    "        F.col(\"violation.violation_details\").alias(\"VIOLATION_DETAILS\"),\n",
    "        F.col(\"violation.violation_memo\").alias(\"VIOLATION_MEMO\"),\n",
    "        F.col(\"violation.violation_points\").alias(\"VIOLATION_POINTS\")\n",
    "    ])\n",
    "    \n",
    "    df_violations = df_exploded.select(*select_cols)\n",
    "\n",
    "    df_violations = df_violations.filter(\n",
    "        (F.trim(F.col(\"VIOLATION_DESCRIPTION_TEMP\")) != \"N/A - Not Applicable\") &\n",
    "        (F.trim(F.col(\"VIOLATION_DESCRIPTION_TEMP\")) != \"\") &\n",
    "        (F.trim(F.col(\"VIOLATION_DESCRIPTION_TEMP\")) != \"Unknown\") &\n",
    "        (F.col(\"VIOLATION_DESCRIPTION_TEMP\").isNotNull())\n",
    "    )\n",
    "    \n",
    "    # EXTRACT VIOLATION CODE AND DESCRIPTION\n",
    "    df_violations = df_violations.withColumn(\"DETAILS_CLEAN\",\n",
    "                      F.when((F.col(\"VIOLATION_DETAILS\").isNotNull()) & (F.trim(F.col(\"VIOLATION_DETAILS\")) != \"\") & (F.trim(F.col(\"VIOLATION_DETAILS\")) != \"Unknown\"),\n",
    "                            F.col(\"VIOLATION_DETAILS\")).otherwise(F.lit(\"9999 Other Violations\")))\n",
    "    \n",
    "    df_violations = df_violations.withColumn(\"CODE_PATTERN_1\", F.regexp_extract(F.col(\"DETAILS_CLEAN\"), r'Ch\\.[\\d.-]+(?:\\([a-zA-Z0-9]+\\))?', 0))\n",
    "    df_violations = df_violations.withColumn(\"CODE_PATTERN_2\", F.regexp_extract(F.col(\"DETAILS_CLEAN\"), r'Sec\\.\\s*[\\d.-]+(?:\\([a-zA-Z0-9]+\\))*', 0))\n",
    "    df_violations = df_violations.withColumn(\"CODE_PATTERN_3\", F.regexp_extract(F.col(\"DETAILS_CLEAN\"), r'[^\\w\\s]*([\\d]+\\.[\\d]+)\\.?', 1))\n",
    "    df_violations = df_violations.withColumn(\"CODE_PATTERN_4\", F.regexp_extract(F.col(\"DETAILS_CLEAN\"), r'^[^\\w\\s]*([\\d]+)', 1))\n",
    "    \n",
    "    df_violations = df_violations.withColumn(\"VIOLATION_CODE\",\n",
    "                      F.coalesce(\n",
    "                          F.when(F.trim(F.col(\"CODE_PATTERN_1\")) != \"\", F.trim(F.col(\"CODE_PATTERN_1\"))),\n",
    "                          F.when(F.trim(F.col(\"CODE_PATTERN_2\")) != \"\", F.trim(F.col(\"CODE_PATTERN_2\"))),\n",
    "                          F.when(F.trim(F.col(\"CODE_PATTERN_3\")) != \"\", F.trim(F.col(\"CODE_PATTERN_3\"))),\n",
    "                          F.when(F.trim(F.col(\"CODE_PATTERN_4\")) != \"\", F.trim(F.col(\"CODE_PATTERN_4\"))),\n",
    "                          F.lit(\"9999\")\n",
    "                      ))\n",
    "    \n",
    "    df_violations = df_violations.drop(\"CODE_PATTERN_1\", \"CODE_PATTERN_2\", \"CODE_PATTERN_3\", \"CODE_PATTERN_4\")\n",
    "    \n",
    "    df_violations = df_violations.withColumn(\"VIOLATION_DESCRIPTION_NEW\",\n",
    "                      F.when(F.col(\"VIOLATION_CODE\") != \"9999\",\n",
    "                            F.expr(\"CASE WHEN INSTR(DETAILS_CLEAN, VIOLATION_CODE) > 0 THEN TRIM(SUBSTRING(DETAILS_CLEAN, INSTR(DETAILS_CLEAN, VIOLATION_CODE) + LENGTH(VIOLATION_CODE))) ELSE DETAILS_CLEAN END\"))\n",
    "                      .otherwise(F.lit(\"Other Violations\")))\n",
    "    \n",
    "    df_violations = df_violations.withColumn(\"VIOLATION_DESCRIPTION_NEW\",\n",
    "                      F.when(F.col(\"VIOLATION_DESCRIPTION_NEW\") != \"Other Violations\",\n",
    "                            F.regexp_replace(F.col(\"VIOLATION_DESCRIPTION_NEW\"), \"^[\\\\s*\\\\)\\\\]#]+\", \"\"))\n",
    "                      .otherwise(F.col(\"VIOLATION_DESCRIPTION_NEW\")))\n",
    "    \n",
    "    df_violations = df_violations.withColumn(\"VIOLATION_DESCRIPTION_NEW\",\n",
    "                      F.when(F.col(\"VIOLATION_DESCRIPTION_NEW\") != \"Other Violations\",\n",
    "                            F.trim(F.regexp_replace(F.col(\"VIOLATION_DESCRIPTION_NEW\"), \"\\\\s+\", \" \")))\n",
    "                      .otherwise(F.col(\"VIOLATION_DESCRIPTION_NEW\")))\n",
    "    \n",
    "    df_violations = df_violations.withColumn(\"VIOLATION_DESCRIPTION_NEW\",\n",
    "                      F.when((F.trim(F.col(\"VIOLATION_DESCRIPTION_NEW\")) == \"\") | (F.col(\"VIOLATION_DESCRIPTION_NEW\").isNull()) | (F.length(F.trim(F.col(\"VIOLATION_DESCRIPTION_NEW\"))) < 3),\n",
    "                            F.lit(\"Other Violations\")).otherwise(F.col(\"VIOLATION_DESCRIPTION_NEW\")))\n",
    "    \n",
    "    df_violations = df_violations.withColumn(\"VIOLATION_CODE\",\n",
    "                      F.when(F.col(\"VIOLATION_DESCRIPTION_NEW\") == \"Other Violations\", F.lit(\"9999\")).otherwise(F.col(\"VIOLATION_CODE\")))\n",
    "    \n",
    "    # Convert VIOLATION_DESCRIPTION_NEW to uppercase and rename to VIOLATION_DESCRIPTION\n",
    "    df_violations = df_violations.withColumn(\"VIOLATION_DESCRIPTION\", F.upper(F.col(\"VIOLATION_DESCRIPTION_NEW\")))\n",
    "    \n",
    "    df_violations = df_violations.drop(\"DETAILS_CLEAN\", \"VIOLATION_DESCRIPTION_TEMP\", \"VIOLATION_DESCRIPTION_NEW\")\n",
    "    \n",
    "    # SPLIT LOCATION INTO LATITUDE AND LONGITUDE\n",
    "    df_violations = df_violations.withColumn(\"LATITUDE\", F.regexp_extract(F.col(\"LOCATION\"), r'\\(([0-9.-]+),\\s*([0-9.-]+)\\)', 1).cast(DoubleType()))\n",
    "    df_violations = df_violations.withColumn(\"LONGITUDE\", F.regexp_extract(F.col(\"LOCATION\"), r'\\(([0-9.-]+),\\s*([0-9.-]+)\\)', 2).cast(DoubleType()))\n",
    "    \n",
    "    zip_window = Window.partitionBy(\"ZIP_CODE\")\n",
    "    df_violations = df_violations.withColumn(\"ZIP_LAT_MEAN\", F.avg(\"LATITUDE\").over(zip_window))\n",
    "    df_violations = df_violations.withColumn(\"ZIP_LON_MEAN\", F.avg(\"LONGITUDE\").over(zip_window))\n",
    "    \n",
    "    df_violations = df_violations.withColumn(\"LATITUDE\",\n",
    "                      F.when(F.col(\"LATITUDE\").isNull(), F.coalesce(F.col(\"ZIP_LAT_MEAN\"), F.lit(32.7767))).otherwise(F.col(\"LATITUDE\")))\n",
    "    df_violations = df_violations.withColumn(\"LONGITUDE\",\n",
    "                      F.when(F.col(\"LONGITUDE\").isNull(), F.coalesce(F.col(\"ZIP_LON_MEAN\"), F.lit(-96.7970))).otherwise(F.col(\"LONGITUDE\")))\n",
    "    \n",
    "    df_violations = df_violations.drop(\"LOCATION\", \"ZIP_LAT_MEAN\", \"ZIP_LON_MEAN\")\n",
    "    \n",
    "    # ADD INSPECTION_RESULT based on score ranges from the image\n",
    "    df_violations = df_violations.withColumn(\"INSPECTION_RESULT\",\n",
    "                      F.when(F.col(\"INSPECTION_SCORE\").between(90, 100), F.lit(\"Very Good\"))\n",
    "                      .when(F.col(\"INSPECTION_SCORE\").between(80, 89), F.lit(\"Good\"))\n",
    "                      .when(F.col(\"INSPECTION_SCORE\").between(70, 79), F.lit(\"Passing\"))\n",
    "                      .when(F.col(\"INSPECTION_SCORE\").between(60, 69), F.lit(\"Failing\"))\n",
    "                      .when(F.col(\"INSPECTION_SCORE\") < 60, F.lit(\"Unacceptable\"))\n",
    "                      .otherwise(F.lit(\"Unknown\")))\n",
    "    \n",
    "    # DROP columns as per requirements\n",
    "    columns_to_drop = [\"VIOLATION_DETAILS\", \"VIOLATION_MEMO\", \"VIOLATION_POINTS\"]\n",
    "    for col in columns_to_drop:\n",
    "        if col in df_violations.columns:\n",
    "            df_violations = df_violations.drop(col)\n",
    "    \n",
    "    if \"LATITUDE\" not in df_violations.columns or \"LONGITUDE\" not in df_violations.columns:\n",
    "        raise Exception(\"LATITUDE or LONGITUDE column missing after splitting LOCATION!\")\n",
    "    \n",
    "    # FINAL COLUMN ORDER (VIOLATION_NUMBER removed)\n",
    "    ordered_cols = [\n",
    "        \"INSPECTION_ID\", \"LICENSE_NUMBER\", \"BUSINESS_NAME\", \"AKA_NAME\", \"FACILITY_TYPE\", \n",
    "        \"INSPECTION_DATE\", \"INSPECTION_TYPE\", \"STREET_ADDRESS\", \"CITY\", \"STATE\", \"ZIP_CODE\", \n",
    "        \"LATITUDE\", \"LONGITUDE\", \"INSPECTION_SCORE\", \"INSPECTION_RESULT\", \"RISK_LEVEL\",\n",
    "        \"VIOLATION_CODE\", \"VIOLATION_DESCRIPTION\"\n",
    "    ]\n",
    "    \n",
    "    remaining_cols = [col for col in df_violations.columns if col not in ordered_cols]\n",
    "    final_cols = [col for col in ordered_cols if col in df_violations.columns] + remaining_cols\n",
    "    df_violations = df_violations.select(*final_cols)\n",
    "\n",
    "    return df_violations\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Bronze_2_Silver_Load_Dallas",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
